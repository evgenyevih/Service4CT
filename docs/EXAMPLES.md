# –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è - Service4CT

## –û–±–∑–æ—Ä

–î–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Service4CT –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö —Ç–æ–º–æ–≥—Ä–∞—Ñ–∏–π –æ—Ä–≥–∞–Ω–æ–≤ –≥—Ä—É–¥–Ω–æ–π –∫–ª–µ—Ç–∫–∏.

## üöÄ –ë—ã—Å—Ç—Ä—ã–µ –ø—Ä–∏–º–µ—Ä—ã

### –ü—Ä–∏–º–µ—Ä 1: –ë–∞–∑–æ–≤—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å

```python
#!/usr/bin/env python3
"""
–ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Service4CT –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ö–¢
"""

import os
import sys
sys.path.append('src')

from src.pipelines.infer_pipeline import run_inference

def main():
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π
    input_dir = "data/input_zips"
    output_path = "data/results/basic_inference.xlsx"
    config_path = "configs/config.yaml"
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if not os.path.exists(input_dir):
        print(f"‚ùå –ü–∞–ø–∫–∞ {input_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return
    
    if not os.listdir(input_dir):
        print(f"‚ùå –ü–∞–ø–∫–∞ {input_dir} –ø—É—Å—Ç–∞")
        return
    
    print("üîç –ó–∞–ø—É—Å–∫ –±–∞–∑–æ–≤–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞...")
    
    try:
        # –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        run_inference(input_dir, output_path, config_path)
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_path}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()
```

### –ü—Ä–∏–º–µ—Ä 2: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

```python
#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ Service4CT
"""

import os
import sys
sys.path.append('src')

from src.pipelines.train_pipeline import run_training

def main():
    config_path = "configs/config.yaml"
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    if not os.path.exists("data/training_data.csv"):
        print("‚ùå –§–∞–π–ª training_data.csv –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    if not os.path.exists("data/training_data"):
        print("‚ùå –ü–∞–ø–∫–∞ training_data –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return
    
    print("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...")
    
    try:
        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        model, metrics = run_training(config_path)
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìä –ú–µ—Ç—Ä–∏–∫–∏: {metrics}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()
```

## üîß –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã

### –ü—Ä–∏–º–µ—Ä 3: –†–∞–±–æ—Ç–∞ —Å –º–æ–¥–µ–ª—å—é –Ω–∞–ø—Ä—è–º—É—é

```python
#!/usr/bin/env python3
"""
–ü—Ä—è–º–∞—è —Ä–∞–±–æ—Ç–∞ —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
"""

import numpy as np
import torch
from src.models.cnn3d import CNN3DModel

def load_and_predict():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    model = CNN3DModel(
        checkpoint_path="weights/best_model.pth",
        device="cpu",  # –∏–ª–∏ "cuda" –¥–ª—è GPU
        depth_size=64,
        spatial_size=64
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ 3D –æ–±—ä–µ–º–∞
    test_volume = np.random.rand(64, 64, 64).astype(np.float32)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    probability = model.predict(test_volume)
    
    print(f"üìä –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–∞—Ç–æ–ª–æ–≥–∏–∏: {probability:.4f}")
    print(f"üéØ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {'–ü–∞—Ç–æ–ª–æ–≥–∏—è' if probability >= 0.5 else '–ù–æ—Ä–º–∞'}")
    
    return probability

def batch_prediction():
    """–ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
    
    model = CNN3DModel(checkpoint_path="weights/best_model.pth")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –±–∞—Ç—á–∞ 3D –æ–±—ä–µ–º–æ–≤
    batch_volumes = [
        np.random.rand(64, 64, 64).astype(np.float32) for _ in range(5)
    ]
    
    # –ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    probabilities = model.predict_batch(batch_volumes)
    
    print("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:")
    for i, prob in enumerate(probabilities):
        print(f"  –û–±—ä–µ–º {i+1}: {prob:.4f} ({'–ü–∞—Ç–æ–ª–æ–≥–∏—è' if prob >= 0.5 else '–ù–æ—Ä–º–∞'})")

if __name__ == "__main__":
    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    load_and_predict()
    print("\nüì¶ –ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:")
    batch_prediction()
```

### –ü—Ä–∏–º–µ—Ä 4: –û–±—Ä–∞–±–æ—Ç–∫–∞ DICOM —Ñ–∞–π–ª–æ–≤

```python
#!/usr/bin/env python3
"""
–û–±—Ä–∞–±–æ—Ç–∫–∞ DICOM —Ñ–∞–π–ª–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
"""

import os
import sys
sys.path.append('src')

from src.io.dicom_io import extract_zip, find_dicom_files, group_by_series
from src.utils.preprocess import series_to_normalized_slices
from src.models.cnn3d import CNN3DModel

def process_dicom_zip(zip_path, output_dir):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ ZIP –∞—Ä—Ö–∏–≤–∞ —Å DICOM —Ñ–∞–π–ª–∞–º–∏"""
    
    print(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ {zip_path}...")
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ZIP –∞—Ä—Ö–∏–≤–∞
    extract_dir = os.path.join(output_dir, "extracted")
    extract_zip(zip_path, extract_dir)
    
    # –ü–æ–∏—Å–∫ DICOM —Ñ–∞–π–ª–æ–≤
    dicom_files = find_dicom_files(extract_dir)
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ DICOM —Ñ–∞–π–ª–æ–≤: {len(dicom_files)}")
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–µ—Ä–∏—è–º
    series_groups = group_by_series(dicom_files)
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Å–µ—Ä–∏–π: {len(series_groups)}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    model = CNN3DModel(checkpoint_path="weights/best_model.pth")
    
    results = []
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–π —Å–µ—Ä–∏–∏
    for series_uid, series_files in series_groups.items():
        print(f"üî¨ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–µ—Ä–∏–∏ {series_uid}...")
        
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Å—Ä–µ–∑—ã
            slices = series_to_normalized_slices(
                series_files,
                num_slices=64,
                window=(-1000, 400)
            )
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            probability = model.predict(slices)
            
            result = {
                'series_uid': series_uid,
                'probability_of_pathology': float(probability),
                'pathology': int(probability >= 0.5),
                'num_slices': len(series_files),
                'status': 'success'
            }
            
            results.append(result)
            print(f"  ‚úÖ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–∞—Ç–æ–ª–æ–≥–∏–∏: {probability:.4f}")
            
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            results.append({
                'series_uid': series_uid,
                'probability_of_pathology': 0.0,
                'pathology': 0,
                'num_slices': len(series_files),
                'status': 'error',
                'error': str(e)
            })
    
    return results

def main():
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π
    zip_path = "data/input_zips/study.zip"
    output_dir = "data/workdir"
    
    if not os.path.exists(zip_path):
        print(f"‚ùå –§–∞–π–ª {zip_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    os.makedirs(output_dir, exist_ok=True)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    results = process_dicom_zip(zip_path, output_dir)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    import pandas as pd
    df = pd.DataFrame(results)
    output_file = "data/results/dicom_analysis.xlsx"
    df.to_excel(output_file, index=False)
    
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
    print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–µ—Ä–∏–π: {len(results)}")
    print(f"üéØ –ü–∞—Ç–æ–ª–æ–≥–∏–π –≤—ã—è–≤–ª–µ–Ω–æ: {sum(r['pathology'] for r in results)}")

if __name__ == "__main__":
    main()
```

### –ü—Ä–∏–º–µ—Ä 5: –ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

```python
#!/usr/bin/env python3
"""
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
"""

import yaml
import os
import sys
sys.path.append('src')

from src.pipelines.infer_pipeline import run_inference

def create_custom_config():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    
    config = {
        'project': {
            'name': 'Service4CT'
        },
        'inference': {
            'num_slices': 128,  # –£–≤–µ–ª–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ä–µ–∑–æ–≤
            'window': [-1200, 600],  # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ HU
            'normalize': True,
            'threshold': 0.3  # –ë–æ–ª–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
        },
        'training': {
            'batch_size': 16,  # –ú–µ–Ω—å—à–∏–π batch –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            'lr': 5e-5,  # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            'epochs': 50,
            'weight_decay': 1e-3,
            'early_stopping_patience': 15,
            'dropout_rate': 0.4,  # –ë–æ–ª—å—à–µ dropout
            'test_size': 0.15,
            'val_size': 0.15,
            'random_state': 42,
            'n_bootstrap': 500,
            'confidence_level': 0.90,
            'focal_loss_alpha': 0.8,
            'focal_loss_gamma': 2.5,
            'class_weights': True,
            'use_weighted_sampler': True,
            'optimize_threshold': True,
            'series_dir': 'data/training_data',
            'data_file': 'training_data.csv'
        },
        'model': {
            'num_classes': 2,
            'checkpoint_path': 'weights/best_model.pth'
        },
        'logging': {
            'level': 'DEBUG'  # –ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        }
    }
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config_path = "configs/custom_config.yaml"
    with open(config_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True, indent=2)
    
    print(f"‚úÖ –ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {config_path}")
    return config_path

def run_with_custom_config():
    """–ó–∞–ø—É—Å–∫ —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config_path = create_custom_config()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π
    input_dir = "data/input_zips"
    output_path = "data/results/custom_inference.xlsx"
    
    print("üîç –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π...")
    
    try:
        run_inference(input_dir, output_path, config_path)
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_path}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    run_with_custom_config()
```

## üê≥ Docker –ø—Ä–∏–º–µ—Ä—ã

### –ü—Ä–∏–º–µ—Ä 6: Docker –∏–Ω—Ñ–µ—Ä–µ–Ω—Å

```bash
#!/bin/bash
# –ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —á–µ—Ä–µ–∑ Docker

# –°–±–æ—Ä–∫–∞ –æ–±—Ä–∞–∑–∞
echo "üî® –°–±–æ—Ä–∫–∞ Docker –æ–±—Ä–∞–∑–∞..."
docker build -t service4ct:latest .

# –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
echo "üîç –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞..."
docker run --rm \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/weights:/app/weights \
  -v $(pwd)/configs:/app/configs \
  service4ct:latest \
  python -m src.main --mode infer \
  --input_dir data/input_zips \
  --output_path data/results/docker_inference.xlsx

echo "‚úÖ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à–µ–Ω!"
```

### –ü—Ä–∏–º–µ—Ä 7: Docker Compose

```yaml
# docker-compose.custom.yml
version: '3.8'

services:
  inference:
    build: .
    container_name: service4ct-inference
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./configs:/app/configs
      - ./weights:/app/weights
    environment:
      - CUDA_VISIBLE_DEVICES=0  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU 0
      - PYTHONPATH=/app
      - LOG_LEVEL=DEBUG
    command: bash -lc "python -m src.main --mode infer --input_dir data/input_zips --output_path data/results/compose_inference.xlsx"
    restart: "no"
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
```

```bash
# –ó–∞–ø—É—Å–∫ —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
docker compose -f docker-compose.custom.yml up inference
```

## üìä –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### –ü—Ä–∏–º–µ—Ä 8: –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞

```python
#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix

def analyze_results(results_file):
    """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞"""
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    df = pd.read_excel(results_file)
    
    print("üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  –í—Å–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π: {len(df)}")
    print(f"  –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(df[df['processing_status'] == 'success'])}")
    print(f"  –û—à–∏–±–æ–∫: {len(df[df['processing_status'] == 'error'])}")
    
    # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç–æ–ª–æ–≥–∏–π
    pathology_count = df['pathology'].sum()
    normal_count = len(df) - pathology_count
    
    print(f"\nüéØ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:")
    print(f"  –ü–∞—Ç–æ–ª–æ–≥–∏–∏: {pathology_count} ({pathology_count/len(df)*100:.1f}%)")
    print(f"  –ù–æ—Ä–º–∞: {normal_count} ({normal_count/len(df)*100:.1f}%)")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    prob_stats = df['probability_of_pathology'].describe()
    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π:")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ: {prob_stats['mean']:.4f}")
    print(f"  –ú–µ–¥–∏–∞–Ω–∞: {prob_stats['50%']:.4f}")
    print(f"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {prob_stats['std']:.4f}")
    print(f"  –ú–∏–Ω–∏–º—É–º: {prob_stats['min']:.4f}")
    print(f"  –ú–∞–∫—Å–∏–º—É–º: {prob_stats['max']:.4f}")
    
    return df

def plot_results(df, output_dir="plots"):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    
    os.makedirs(output_dir, exist_ok=True)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    plt.figure(figsize=(10, 6))
    plt.hist(df['probability_of_pathology'], bins=20, alpha=0.7, edgecolor='black')
    plt.axvline(0.5, color='red', linestyle='--', label='–ü–æ—Ä–æ–≥ 0.5')
    plt.xlabel('–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–∞—Ç–æ–ª–æ–≥–∏–∏')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π')
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø–∞—Ç–æ–ª–æ–≥–∏–∏')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(f'{output_dir}/probability_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # Box plot –ø–æ –∫–ª–∞—Å—Å–∞–º
    plt.figure(figsize=(8, 6))
    df.boxplot(column='probability_of_pathology', by='pathology', ax=plt.gca())
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø–æ –∫–ª–∞—Å—Å–∞–º')
    plt.xlabel('–ö–ª–∞—Å—Å (0=–ù–æ—Ä–º–∞, 1=–ü–∞—Ç–æ–ª–æ–≥–∏—è)')
    plt.ylabel('–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–∞—Ç–æ–ª–æ–≥–∏–∏')
    plt.savefig(f'{output_dir}/class_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"üìä –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_dir}/")

def main():
    results_file = "data/results/results.xlsx"
    
    if not os.path.exists(results_file):
        print(f"‚ùå –§–∞–π–ª {results_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    print("üìä –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞...")
    
    # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
    df = analyze_results(results_file)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plot_results(df)
    
    print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    main()
```

### –ü—Ä–∏–º–µ—Ä 9: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

```python
#!/usr/bin/env python3
"""
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
"""

import pandas as pd
import numpy as np
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc

def compare_models(results_files, model_names):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    
    results = {}
    
    for file, name in zip(results_files, model_names):
        if os.path.exists(file):
            df = pd.read_excel(file)
            results[name] = df
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è {name}")
        else:
            print(f"‚ùå –§–∞–π–ª {file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    print("\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:")
    print("-" * 50)
    
    for name, df in results.items():
        if 'probability_of_pathology' in df.columns and 'pathology' in df.columns:
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            valid_df = df[df['processing_status'] == 'success']
            
            if len(valid_df) > 0:
                y_true = valid_df['pathology']
                y_pred = valid_df['probability_of_pathology']
                
                # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
                auc_score = roc_auc_score(y_true, y_pred)
                precision, recall, _ = precision_recall_curve(y_true, y_pred)
                pr_auc = auc(recall, precision)
                
                print(f"{name}:")
                print(f"  AUC-ROC: {auc_score:.4f}")
                print(f"  AUC-PR: {pr_auc:.4f}")
                print(f"  –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π: {len(valid_df)}")
                print()

def main():
    # –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    results_files = [
        "data/results/results.xlsx",
        "data/results/custom_inference.xlsx",
        "data/results/docker_inference.xlsx"
    ]
    
    model_names = [
        "–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å",
        "–ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è",
        "Docker –º–æ–¥–µ–ª—å"
    ]
    
    print("üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
    compare_models(results_files, model_names)

if __name__ == "__main__":
    main()
```

## üîß –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–Ω–µ—à–Ω–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏

### –ü—Ä–∏–º–µ—Ä 10: REST API –æ–±–µ—Ä—Ç–∫–∞

```python
#!/usr/bin/env python3
"""
REST API –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è Service4CT
"""

from flask import Flask, request, jsonify, send_file
import os
import tempfile
import zipfile
from src.pipelines.infer_pipeline import run_inference

app = Flask(__name__)

@app.route('/health', methods=['GET'])
def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': os.path.exists('weights/best_model.pth'),
        'config_exists': os.path.exists('configs/config.yaml')
    })

@app.route('/analyze', methods=['POST'])
def analyze_ct():
    """–ê–Ω–∞–ª–∏–∑ –ö–¢ —Ñ–∞–π–ª–æ–≤"""
    
    if 'file' not in request.files:
        return jsonify({'error': 'No file provided'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No file selected'}), 400
    
    if not file.filename.endswith('.zip'):
        return jsonify({'error': 'Only ZIP files are supported'}), 400
    
    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        with tempfile.TemporaryDirectory() as temp_dir:
            input_dir = os.path.join(temp_dir, 'input')
            output_path = os.path.join(temp_dir, 'results.xlsx')
            
            os.makedirs(input_dir, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            zip_path = os.path.join(input_dir, file.filename)
            file.save(zip_path)
            
            # –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
            run_inference(input_dir, output_path, 'configs/config.yaml')
            
            # –ß—Ç–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            import pandas as pd
            df = pd.read_excel(output_path)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ JSON
            results = df.to_dict('records')
            
            return jsonify({
                'status': 'success',
                'results': results,
                'total_studies': len(results),
                'pathologies_detected': sum(r['pathology'] for r in results)
            })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/results/<filename>')
def download_results(filename):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    results_path = f'data/results/{filename}'
    if os.path.exists(results_path):
        return send_file(results_path, as_attachment=True)
    else:
        return jsonify({'error': 'File not found'}), 404

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)
```

### –ü—Ä–∏–º–µ—Ä 11: Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞

```python
#!/usr/bin/env python3
"""
Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
"""

import os
import glob
import concurrent.futures
from src.pipelines.infer_pipeline import run_inference

def process_single_zip(zip_path, output_dir):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ ZIP —Ñ–∞–π–ª–∞"""
    
    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–π –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        zip_name = os.path.splitext(os.path.basename(zip_path))[0]
        zip_output_dir = os.path.join(output_dir, zip_name)
        os.makedirs(zip_output_dir, exist_ok=True)
        
        # –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        output_path = os.path.join(zip_output_dir, 'results.xlsx')
        run_inference(
            input_dir=os.path.dirname(zip_path),
            output_path=output_path,
            config_path='configs/config.yaml'
        )
        
        return {
            'zip_path': zip_path,
            'output_path': output_path,
            'status': 'success'
        }
        
    except Exception as e:
        return {
            'zip_path': zip_path,
            'error': str(e),
            'status': 'error'
        }

def batch_process(input_dir, output_dir, max_workers=4):
    """Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    
    # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö ZIP —Ñ–∞–π–ª–æ–≤
    zip_files = glob.glob(os.path.join(input_dir, '*.zip'))
    
    if not zip_files:
        print(f"‚ùå ZIP —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {input_dir}")
        return
    
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ ZIP —Ñ–∞–π–ª–æ–≤: {len(zip_files)}")
    print(f"üë• –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Ç–æ–∫–æ–≤: {max_workers}")
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # –ó–∞–ø—É—Å–∫ –∑–∞–¥–∞—á
        future_to_zip = {
            executor.submit(process_single_zip, zip_path, output_dir): zip_path 
            for zip_path in zip_files
        }
        
        # –°–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        for future in concurrent.futures.as_completed(future_to_zip):
            zip_path = future_to_zip[future]
            try:
                result = future.result()
                results.append(result)
                
                if result['status'] == 'success':
                    print(f"‚úÖ {os.path.basename(zip_path)}")
                else:
                    print(f"‚ùå {os.path.basename(zip_path)}: {result['error']}")
                    
            except Exception as e:
                print(f"‚ùå {os.path.basename(zip_path)}: {e}")
                results.append({
                    'zip_path': zip_path,
                    'error': str(e),
                    'status': 'error'
                })
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    successful = sum(1 for r in results if r['status'] == 'success')
    failed = len(results) - successful
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
    print(f"  –£—Å–ø–µ—à–Ω–æ: {successful}")
    print(f"  –û—à–∏–±–æ–∫: {failed}")
    print(f"  –í—Å–µ–≥–æ: {len(results)}")
    
    return results

def main():
    input_dir = "data/input_zips"
    output_dir = "data/batch_results"
    
    if not os.path.exists(input_dir):
        print(f"‚ùå –ü–∞–ø–∫–∞ {input_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return
    
    print("üîÑ –ó–∞–ø—É—Å–∫ batch –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
    results = batch_process(input_dir, output_dir, max_workers=2)
    
    if results:
        print("‚úÖ Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

if __name__ == "__main__":
    main()
```

## üìã –ß–µ–∫-–ª–∏—Å—Ç –ø—Ä–∏–º–µ—Ä–æ–≤

### –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã

- [ ] –ü—Ä–∏–º–µ—Ä 1: –ë–∞–∑–æ–≤—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
- [ ] –ü—Ä–∏–º–µ—Ä 2: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
- [ ] –ü—Ä–∏–º–µ—Ä 3: –†–∞–±–æ—Ç–∞ —Å –º–æ–¥–µ–ª—å—é –Ω–∞–ø—Ä—è–º—É—é

### –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã

- [ ] –ü—Ä–∏–º–µ—Ä 4: –û–±—Ä–∞–±–æ—Ç–∫–∞ DICOM —Ñ–∞–π–ª–æ–≤
- [ ] –ü—Ä–∏–º–µ—Ä 5: –ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
- [ ] –ü—Ä–∏–º–µ—Ä 6: Docker –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
- [ ] –ü—Ä–∏–º–µ—Ä 7: Docker Compose

### –ê–Ω–∞–ª–∏–∑ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

- [ ] –ü—Ä–∏–º–µ—Ä 8: –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- [ ] –ü—Ä–∏–º–µ—Ä 9: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
- [ ] –ü—Ä–∏–º–µ—Ä 10: REST API –æ–±–µ—Ä—Ç–∫–∞
- [ ] –ü—Ä–∏–º–µ—Ä 11: Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞

---

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ**: –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –≥–æ—Ç–æ–≤—ã –∫ –∑–∞–ø—É—Å–∫—É –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫. –ê–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –ø—É—Ç–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥ –≤–∞—à—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é.
