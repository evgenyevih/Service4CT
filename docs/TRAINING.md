# –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–∏ - Service4CT

## –û–±–∑–æ—Ä

–î–∞–Ω–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è 3D CNN –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö —Ç–æ–º–æ–≥—Ä–∞—Ñ–∏–π –æ—Ä–≥–∞–Ω–æ–≤ –≥—Ä—É–¥–Ω–æ–π –∫–ª–µ—Ç–∫–∏. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–Ω–æ—Ä–º–∞/–ø–∞—Ç–æ–ª–æ–≥–∏—è).

## üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö

```
data/
‚îú‚îÄ‚îÄ training_data/           # DICOM —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ study_001/           # –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ 1
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ series_001/      # –°–µ—Ä–∏—è 1
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ series_002/      # –°–µ—Ä–∏—è 2
‚îÇ   ‚îî‚îÄ‚îÄ study_002/           # –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ 2
‚îî‚îÄ‚îÄ training_data.csv        # –§–∞–π–ª —Å –º–µ—Ç–∫–∞–º–∏
```

### –§–æ—Ä–º–∞—Ç –º–µ—Ç–æ–∫ (training_data.csv)

```csv
study_uid,series_uid,pathology,age,sex
1.2.3.4.5.6.7.8.9.10,1.2.3.4.5.6.7.8.9.11,1,65,M
1.2.3.4.5.6.7.8.9.12,1.2.3.4.5.6.7.8.9.13,0,45,F
```

**–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏:**
- `study_uid`: –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
- `series_uid`: –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–µ—Ä–∏–∏
- `pathology`: –ú–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∞ (0=–Ω–æ—Ä–º–∞, 1=–ø–∞—Ç–æ–ª–æ–≥–∏—è)

**–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏:**
- `age`: –í–æ–∑—Ä–∞—Å—Ç –ø–∞—Ü–∏–µ–Ω—Ç–∞
- `sex`: –ü–æ–ª –ø–∞—Ü–∏–µ–Ω—Ç–∞ (M/F)

### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ DICOM —Ñ–∞–π–ª–∞–º

1. **–§–æ—Ä–º–∞—Ç**: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ DICOM —Ñ–∞–π–ª—ã
2. **–†–∞–∑–º–µ—Ä**: –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 512x512 –ø–∏–∫—Å–µ–ª–µ–π
3. **–¢–æ–ª—â–∏–Ω–∞ —Å—Ä–µ–∑–∞**: 1-5 –º–º
4. **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ä–µ–∑–æ–≤**: –ú–∏–Ω–∏–º—É–º 32, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 64+
5. **–ú–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å**: CT (Computed Tomography)

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏

### CNN3D - 3D Convolutional Neural Network

```python
class CNN3D(nn.Module):
    def __init__(self, num_classes=2, dropout_rate=0.3):
        super(CNN3D, self).__init__()
        
        # 3D Convolutional layers
        self.conv1 = nn.Conv3d(1, 64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv3d(64, 128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv3d(128, 256, kernel_size=3, padding=1)
        
        # Batch normalization
        self.bn1 = nn.BatchNorm3d(64)
        self.bn2 = nn.BatchNorm3d(128)
        self.bn3 = nn.BatchNorm3d(256)
        
        # Pooling
        self.pool = nn.MaxPool3d(2, 2)
        
        # Dropout
        self.dropout = nn.Dropout3d(dropout_rate)
        
        # Fully connected layers
        self.fc1 = nn.Linear(256 * 8 * 8 * 8, 512)
        self.fc2 = nn.Linear(512, num_classes)
```

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

- **3D —Å–≤–µ—Ä—Ç–∫–∏**: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ä–µ–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- **Batch Normalization**: –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
- **Dropout**: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- **Residual connections**: –£–ª—É—á—à–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞

## ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è

### –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

```yaml
# configs/config.yaml
training:
  batch_size: 32                    # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
  lr: 1e-4                         # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
  epochs: 100                       # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
  weight_decay: 1e-4               # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
  early_stopping_patience: 20       # –¢–µ—Ä–ø–µ–Ω–∏–µ –¥–ª—è early stopping
  dropout_rate: 0.3                 # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç dropout
  
  # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
  test_size: 0.2                    # –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞
  val_size: 0.2                     # –î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞
  random_state: 42                  # –°–ª—É—á–∞–π–Ω–æ–µ –∑–µ—Ä–Ω–æ
  
  # –ú–µ—Ç—Ä–∏–∫–∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è
  n_bootstrap: 1000                 # Bootstrap –≤—ã–±–æ—Ä–∫–∏
  confidence_level: 0.95            # –£—Ä–æ–≤–µ–Ω—å –¥–æ–≤–µ—Ä–∏—è
  
  # –ü–æ—Ç–µ—Ä—è –∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞
  focal_loss_alpha: 0.75           # –ê–ª—å—Ñ–∞ –¥–ª—è Focal Loss
  focal_loss_gamma: 2.0           # –ì–∞–º–º–∞ –¥–ª—è Focal Loss
  class_weights: true               # –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
  use_weighted_sampler: true        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
  
  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞
  optimize_threshold: true          # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
  
  # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
  series_dir: "data/training_data"  # –ü–∞–ø–∫–∞ —Å DICOM –¥–∞–Ω–Ω—ã–º–∏
  data_file: "training_data.csv"    # –§–∞–π–ª —Å –º–µ—Ç–∫–∞–º–∏
```

### –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

#### 1. –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö

```python
# –ü—Ä–∏–º–µ—Ä –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤ train_model.py
transforms = [
    RandomRotation3D(degrees=10),
    RandomFlip3D(axis=2),  # –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ
    RandomNoise3D(noise_factor=0.1),
    RandomBrightness3D(brightness_factor=0.2)
]
```

#### 2. Learning Rate Scheduler

```python
# –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, 
    mode='min', 
    factor=0.5, 
    patience=5,
    verbose=True
)
```

## üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è

### –õ–æ–∫–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

```bash
# –ê–∫—Ç–∏–≤–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è
conda activate service4ct

# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
python -m src.main --mode train

# –ò–ª–∏ —á–µ—Ä–µ–∑ —Å–∫—Ä–∏–ø—Ç
./scripts/train.sh
```

### Docker –æ–±—É—á–µ–Ω–∏–µ

```bash
# –ó–∞–ø—É—Å–∫ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
docker compose run --rm training

# –ò–ª–∏ —Å GPU
docker run --gpus all -v $(pwd)/data:/app/data -v $(pwd)/weights:/app/weights service4ct:latest python -m src.main --mode train
```

### –û–±—É—á–µ–Ω–∏–µ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏

```python
# –ü—Ä—è–º–æ–π –≤—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è
from src.training.train_model import train_model

# –û–±—É—á–µ–Ω–∏–µ —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
model, metrics = train_model("configs/custom_config.yaml")
```

## üìà –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è

### –ú–µ—Ç—Ä–∏–∫–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏

```python
# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
import wandb

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Weights & Biases
wandb.init(project="service4ct", name="experiment_1")

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
wandb.log({
    "epoch": epoch,
    "train_loss": train_loss,
    "val_loss": val_loss,
    "val_auc": val_auc,
    "learning_rate": optimizer.param_groups[0]['lr']
})
```

### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞

```python
# ROC –∫—Ä–∏–≤—ã–µ
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

fpr, tpr, _ = roc_curve(y_true, y_pred)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.savefig('plots/roc_curve.png')
```

## üéØ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

### Grid Search

```python
# –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
param_grid = {
    'lr': [1e-5, 1e-4, 1e-3],
    'batch_size': [16, 32, 64],
    'dropout_rate': [0.2, 0.3, 0.4],
    'weight_decay': [1e-5, 1e-4, 1e-3]
}

best_params = grid_search(param_grid, X_train, y_train)
```

### Bayesian Optimization

```python
# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é Optuna
import optuna

def objective(trial):
    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])
    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)
    
    model = CNN3D(dropout_rate=dropout_rate)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    # –û–±—É—á–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è
    val_auc = train_and_validate(model, optimizer, batch_size)
    return val_auc

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)
```

## üìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏

### –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

```python
# –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
from sklearn.metrics import classification_report, confusion_matrix

# –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
print(classification_report(y_true, y_pred, target_names=['Normal', 'Pathology']))

# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
cm = confusion_matrix(y_true, y_pred)
print("Confusion Matrix:")
print(cm)
```

### Bootstrap –≤–∞–ª–∏–¥–∞—Ü–∏—è

```python
# –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –¥–ª—è –º–µ—Ç—Ä–∏–∫
def bootstrap_metrics(y_true, y_pred, n_bootstrap=1000):
    metrics = []
    for _ in range(n_bootstrap):
        indices = np.random.choice(len(y_true), len(y_true), replace=True)
        y_true_boot = y_true[indices]
        y_pred_boot = y_pred[indices]
        
        auc = roc_auc_score(y_true_boot, y_pred_boot)
        metrics.append(auc)
    
    return np.percentile(metrics, [2.5, 97.5])  # 95% –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
```

### –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è

```python
# K-fold –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
from sklearn.model_selection import StratifiedKFold

kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = []

for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):
    X_train_fold, X_val_fold = X[train_idx], X[val_idx]
    y_train_fold, y_val_fold = y[train_idx], y[val_idx]
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ fold
    model = CNN3D()
    # ... –æ–±—É—á–µ–Ω–∏–µ ...
    
    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    val_auc = evaluate_model(model, X_val_fold, y_val_fold)
    cv_scores.append(val_auc)
    
    print(f"Fold {fold + 1}: AUC = {val_auc:.4f}")

print(f"CV AUC: {np.mean(cv_scores):.4f} ¬± {np.std(cv_scores):.4f}")
```

## üîß –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

### –ß–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

#### 1. –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (Overfitting)

**–°–∏–º–ø—Ç–æ–º—ã:**
- –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ë–æ–ª—å—à–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É train –∏ val loss

**–†–µ—à–µ–Ω–∏—è:**
```python
# –£–≤–µ–ª–∏—á–µ–Ω–∏–µ dropout
dropout_rate = 0.5

# –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
early_stopping = EarlyStopping(patience=10, restore_best_weights=True)

# –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
weight_decay = 1e-3

# –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
transforms = [RandomRotation3D(), RandomFlip3D(), RandomNoise3D()]
```

#### 2. –ù–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ (Underfitting)

**–°–∏–º–ø—Ç–æ–º—ã:**
- –ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
- –ú–æ–¥–µ–ª—å –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è

**–†–µ—à–µ–Ω–∏—è:**
```python
# –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
hidden_size = 1024

# –£–º–µ–Ω—å—à–µ–Ω–∏–µ dropout
dropout_rate = 0.1

# –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–æ—Ö
epochs = 200

# –£–≤–µ–ª–∏—á–µ–Ω–∏–µ learning rate
lr = 1e-3
```

#### 3. –î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤

**–°–∏–º–ø—Ç–æ–º—ã:**
- –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å
- –ù–∏–∑–∫–∞—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å

**–†–µ—à–µ–Ω–∏—è:**
```python
# –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)

# Focal Loss
focal_loss = FocalLoss(alpha=0.75, gamma=2.0)

# –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
weighted_sampler = WeightedRandomSampler(weights, len(weights))
```

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

#### 1. –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è

```python
# Mixed Precision Training
from torch.cuda.amp import GradScaler, autocast

scaler = GradScaler()

for batch in dataloader:
    with autocast():
        outputs = model(batch)
        loss = criterion(outputs, targets)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

#### 2. –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏

```python
# Gradient Accumulation
accumulation_steps = 4

for i, batch in enumerate(dataloader):
    outputs = model(batch)
    loss = criterion(outputs, targets) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

## üìã –ß–µ–∫-–ª–∏—Å—Ç –æ–±—É—á–µ–Ω–∏—è

### –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞

- [ ] –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –∏ —Ä–∞–∑–º–µ—á–µ–Ω—ã
- [ ] DICOM —Ñ–∞–π–ª—ã –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å
- [ ] CSV —Ñ–∞–π–ª —Å –º–µ—Ç–∫–∞–º–∏ —Å–æ–∑–¥–∞–Ω
- [ ] –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞
- [ ] –û–∫—Ä—É–∂–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–æ

### –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è

- [ ] –ú–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
- [ ] –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã
- [ ] –û–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ
- [ ] –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—é—Ç—Å—è
- [ ] –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç

### –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞

- [ ] –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞
- [ ] –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã
- [ ] –ü–æ—Ä–æ–≥ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω
- [ ] –û—Ç—á–µ—Ç—ã —Å–æ–∑–¥–∞–Ω—ã
- [ ] –ú–æ–¥–µ–ª—å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞

## üìä –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

```python
# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
results = {
    'train_metrics': train_metrics,
    'val_metrics': val_metrics,
    'test_metrics': test_metrics,
    'optimal_threshold': optimal_threshold,
    'training_history': history
}

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
import json
with open('plots/training_results.json', 'w') as f:
    json.dump(results, f, indent=2)
```

### –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫

```python
# –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def analyze_feature_importance(model, dataloader):
    model.eval()
    gradients = []
    
    for batch in dataloader:
        batch.requires_grad_()
        output = model(batch)
        loss = output.sum()
        loss.backward()
        
        gradients.append(batch.grad.numpy())
    
    return np.mean(gradients, axis=0)
```

---

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ**: –î–∞–Ω–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –î–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏, –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö.
