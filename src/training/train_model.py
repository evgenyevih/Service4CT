#!/usr/bin/env python3
"""
Service4CT - —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ö–¢ –æ—Ä–≥–∞–Ω–æ–≤ –≥—Ä—É–¥–Ω–æ–π –∫–ª–µ—Ç–∫–∏.
–í–∫–ª—é—á–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–æ—Ä–æ–≥–∞, —É–ª—É—á—à–µ–Ω–Ω—É—é –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –∫–ª–∞—Å—Å–æ–≤ –∏ Focal Loss.
"""

import os
import warnings
from collections import Counter
from datetime import datetime
from typing import Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from scipy import ndimage
from scipy import stats
from sklearn.metrics import (auc, confusion_matrix, precision_recall_curve,
                             roc_auc_score, roc_curve)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler
from tqdm import tqdm

import pydicom
from pydicom.pixel_data_handlers.util import apply_voi_lut
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore')


# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
ID_COL = 'SeriesInstanceUID'
LABEL_COLS = ['Normal', 'Pathology']
TARGET_SIZE = (64, 64, 64)
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è (–∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
CONFIG = {
    'batch_size': 32,
    'epochs': 30,
    'learning_rate': 1e-4,
    'weight_decay': 1e-4,
    'early_stopping_patience': 20,
    'dropout_rate': 0.3,
    'device': DEVICE,
    'test_size': 0.2,
    'val_size': 0.2,
    'random_state': 42,
    'n_bootstrap': 1000,
    'confidence_level': 0.95,
    'focal_loss_alpha': 0.75,
    'focal_loss_gamma': 2.0,
    'class_weights': True,
    'use_weighted_sampler': True,
    'optimize_threshold': True,
    'series_dir': 'data/training_data',
    'data_file': 'training_data.csv'
}



class DICOMProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä DICOM —Å–µ—Ä–∏–π –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ 3D –æ–±—ä–µ–º—ã"""
    
    def __init__(self, target_size: Tuple[int, int, int] = TARGET_SIZE):
        self.target_size = target_size
        self.scaler = StandardScaler()
    
    def load_dicom_series(self, series_path: str) -> np.ndarray:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ DICOM —Å–µ—Ä–∏–∏ –≤ 3D –æ–±—ä–µ–º"""
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö DICOM —Ñ–∞–π–ª–æ–≤
            dicom_files = []
            for root, _, files in os.walk(series_path):
                for file in files:
                    dicom_files.append(os.path.join(root, file))
            
            if not dicom_files:
                raise ValueError(f"No DICOM files found in {series_path}")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ DICOM —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
            dicoms_with_position = []
            for filepath in dicom_files:
                try:
                    ds = pydicom.dcmread(filepath, force=True)
                    if hasattr(ds, 'PixelData'):
                        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ Z-–ø–æ–∑–∏—Ü–∏–∏ —Å –Ω–∞–¥–µ–∂–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
                        z_pos = None
                        if hasattr(ds, 'ImagePositionPatient') and len(ds.ImagePositionPatient) >= 3:
                            try:
                                z_pos = float(ds.ImagePositionPatient[2])
                            except (ValueError, TypeError):
                                pass
                        dicoms_with_position.append((ds, filepath, z_pos))
                except Exception:
                    continue
            
            if not dicoms_with_position:
                raise ValueError(f"No valid DICOM files with pixel data in {series_path}")
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ Z-–ø–æ–∑–∏—Ü–∏–∏ –∏–ª–∏ InstanceNumber
            valid_slices = [(ds, filepath, z_pos) for ds, filepath, z_pos in dicoms_with_position if z_pos is not None]
            
            if not valid_slices:
                # Fallback –∫ InstanceNumber
                dicoms_with_position.sort(key=lambda x: getattr(x[0], 'InstanceNumber', 0))
                dicoms = [(ds, filepath) for ds, filepath, _ in dicoms_with_position]
            else:
                # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π Z-–ø–æ–∑–∏—Ü–∏–∏
                valid_slices.sort(key=lambda x: x[2])
                dicoms = [(ds, filepath) for ds, filepath, _ in valid_slices]
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ñ–æ—Ä–º—ã
            volume_slices = []
            for ds, _ in dicoms:
                try:
                    # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–∞—Å—Å–∏–≤–∞ –ø–∏–∫—Å–µ–ª–µ–π —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
                    pixel_array = ds.pixel_array.astype(np.float32)
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ñ–æ—Ä–º—ã
                    if len(volume_slices) > 0 and pixel_array.shape != volume_slices[0].shape:
                        continue
                    
                    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ rescale –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
                    if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):
                        slope = float(ds.RescaleSlope)
                        intercept = float(ds.RescaleIntercept)
                        pixel_array = pixel_array * slope + intercept

                    volume_slices.append(pixel_array)
                except Exception:
                    continue
            
            if not volume_slices:
                raise ValueError("No valid slices extracted")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö —Å—Ä–µ–∑–æ–≤ –ø–µ—Ä–µ–¥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º
            if len(set(slice.shape for slice in volume_slices)) > 1:
                shape_counts = Counter(slice.shape for slice in volume_slices)
                target_shape = shape_counts.most_common(1)[0][0]
                volume_slices = [slice for slice in volume_slices if slice.shape == target_shape]
            
            if not volume_slices:
                raise ValueError("No consistent slices found")
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤ 3D –æ–±—ä–µ–º
            volume = np.stack(volume_slices, axis=0)  # Shape: (depth, height, width)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞
            volume = self.preprocess_volume(volume)
            
            return volume
            
        except Exception as e:
            print(f"Error processing series {series_path}: {e}")
            return np.zeros(self.target_size, dtype=np.float32)
    
    def preprocess_volume(self, volume: np.ndarray) -> np.ndarray:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ 3D –æ–±—ä–µ–º–∞: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –æ–±—Ä–µ–∑–∫–∞, –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞"""
        if volume.size == 0:
            return np.zeros(self.target_size, dtype=np.float32)
        
        # –û–±—Ä–µ–∑–∫–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (—É—Å—Ç–æ–π—á–∏–≤–æ –∫ –≤—ã–±—Ä–æ—Å–∞–º)
        p1, p99 = np.percentile(volume, [1, 99])
        volume = np.clip(volume, p1, p99)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ [0, 1]
        volume_min, volume_max = volume.min(), volume.max()
        if volume_max > volume_min:
            volume = (volume - volume_min) / (volume_max - volume_min)
        
        # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–æ —Ü–µ–ª–µ–≤–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        if volume.shape != self.target_size:
            zoom_factors = [
                self.target_size[i] / volume.shape[i] for i in range(3)
            ]
            volume = ndimage.zoom(volume, zoom_factors, order=1)
        
        return volume.astype(np.float32)



class CNN3D(nn.Module):
    """3D CNN —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    def __init__(self, num_classes=2, dropout_rate=0.3):
        super(CNN3D, self).__init__()
        
        # –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å residual connections
        self.conv1 = nn.Conv3d(1, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm3d(32)
        self.pool1 = nn.MaxPool3d(2)
        
        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm3d(64)
        self.pool2 = nn.MaxPool3d(2)
        
        self.conv3 = nn.Conv3d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm3d(128)
        self.pool3 = nn.MaxPool3d(2)
        
        self.conv4 = nn.Conv3d(128, 256, kernel_size=3, padding=1)
        self.bn4 = nn.BatchNorm3d(256)
        self.pool4 = nn.MaxPool3d(2)
        
        # Global average pooling
        self.global_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
        
        # –ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.fc1 = nn.Linear(256, 512)
        self.dropout1 = nn.Dropout(dropout_rate)
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(dropout_rate * 0.8)
        self.fc3 = nn.Linear(256, 128)
        self.dropout3 = nn.Dropout(dropout_rate * 0.6)
        self.fc4 = nn.Linear(128, num_classes)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self._initialize_weights()
    
    def _initialize_weights(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏"""
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm3d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # x: [batch_size, 1, depth, height, width]
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.pool2(x)
        
        x = F.relu(self.bn3(self.conv3(x)))
        x = self.pool3(x)
        
        x = F.relu(self.bn4(self.conv4(x)))
        x = self.pool4(x)
        
        # Global average pooling
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = F.relu(self.fc3(x))
        x = self.dropout3(x)
        x = self.fc4(x)
        
        return torch.sigmoid(x)



class CTDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è –ö–¢"""
    
    def __init__(self, data_df: pd.DataFrame, series_dir: str, processor: DICOMProcessor):
        self.data_df = data_df
        self.series_dir = series_dir
        self.processor = processor
        
    def __len__(self):
        return len(self.data_df)
    
    def __getitem__(self, idx):
        row = self.data_df.iloc[idx]
        series_id = row[ID_COL]
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—ä–µ–º–∞
        series_path = os.path.join(self.series_dir, series_id)
        volume = self.processor.load_dicom_series(series_path)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–æ–∫
        labels = row[LABEL_COLS].values.astype(np.float32)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–∞–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        volume_tensor = torch.from_numpy(volume).unsqueeze(0)
        labels_tensor = torch.from_numpy(labels)
        
        return volume_tensor, labels_tensor



class FocalLoss(nn.Module):
    """Focal Loss –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤"""
    
    def __init__(self, alpha=0.75, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ sigmoid –∫ –≤—Ö–æ–¥–∞–º
        inputs = torch.sigmoid(inputs)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ BCE loss
        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ p_t
        p_t = inputs * targets + (1 - inputs) * (1 - targets)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ alpha_t
        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ focal weight
        focal_weight = alpha_t * (1 - p_t) ** self.gamma
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ focal weight
        focal_loss = focal_weight * bce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss



def find_optimal_threshold(y_true, y_pred_proba):
    """–ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ F1-score"""
    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ F1-score –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ä–æ–≥–∞
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    
    # –ü–æ–∏—Å–∫ –ø–æ—Ä–æ–≥–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º F1-score
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5
    
    return optimal_threshold, f1_scores[optimal_idx]

def calculate_metrics_with_threshold(y_true, y_pred_proba, threshold=0.5):
    """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º"""
    y_pred_binary = (y_pred_proba > threshold).astype(int)
    
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary).ravel()
    
    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    f1_score = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0
    
    return {
        'sensitivity': sensitivity,
        'specificity': specificity,
        'precision': precision,
        'f1_score': f1_score,
        'threshold': threshold,
        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn
    }


def save_training_results(all_metrics, optimal_threshold):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
    from datetime import datetime
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò Service4CT")
    report_lines.append("=" * 80)
    report_lines.append(f"–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ (–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏): {optimal_threshold:.4f}")
    report_lines.append("")
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –Ω–∞–±–æ—Ä–∞–º –¥–∞–Ω–Ω—ã—Ö
    for dataset_name, data in all_metrics.items():
        metrics = data['metrics']
        roc_auc = data['roc_auc']
        
        report_lines.append(f"{'=' * 60}")
        report_lines.append(f"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê {dataset_name.upper()} –ù–ê–ë–û–†–ï")
        report_lines.append(f"{'=' * 60}")
        report_lines.append(f"–ü–æ—Ä–æ–≥: {metrics['threshold']:.4f}")
        report_lines.append(f"AUC-ROC: {roc_auc:.4f}")
        report_lines.append(f"–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {metrics['sensitivity']:.4f}")
        report_lines.append(f"–°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å: {metrics['specificity']:.4f}")
        report_lines.append(f"–¢–æ—á–Ω–æ—Å—Ç—å: {metrics['precision']:.4f}")
        report_lines.append(f"F1-–º–µ—Ä–∞: {metrics['f1_score']:.4f}")
        report_lines.append(f"True Positives: {metrics['tp']}")
        report_lines.append(f"True Negatives: {metrics['tn']}")
        report_lines.append(f"False Positives: {metrics['fp']}")
        report_lines.append(f"False Negatives: {metrics['fn']}")
        report_lines.append("")
    
    # –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    report_lines.append("=" * 80)
    report_lines.append("–°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    report_lines.append("=" * 80)
    report_lines.append(f"{'–ù–∞–±–æ—Ä':<12} {'AUC-ROC':<8} {'Sensitivity':<12} {'Specificity':<12} {'Precision':<10} {'F1-Score':<10}")
    report_lines.append("-" * 80)
    
    for dataset_name, data in all_metrics.items():
        metrics = data['metrics']
        roc_auc = data['roc_auc']
        report_lines.append(f"{dataset_name:<12} {roc_auc:<8.4f} {metrics['sensitivity']:<12.4f} {metrics['specificity']:<12.4f} {metrics['precision']:<10.4f} {metrics['f1_score']:<10.4f}")
    
    report_lines.append("=" * 80)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
    report_content = "\n".join(report_lines)
    report_path = "plots/training_results.txt"
    
    try:
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        print(f"üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {report_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {e}")



def create_weighted_sampler(train_df):
    """–°–æ–∑–¥–∞–µ—Ç –≤–∑–≤–µ—à–µ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤"""
    class_counts = train_df['Pathology'].value_counts()
    class_weights = 1.0 / class_counts.values
    sample_weights = [class_weights[train_df.iloc[i]['Pathology']] for i in range(len(train_df))]
    
    return WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(train_df),
        replacement=True
    )

def calculate_class_weights(train_df):
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏"""
    class_counts = train_df['Pathology'].value_counts()
    total_samples = len(train_df)
    
    # –û–±—Ä–∞—Ç–Ω–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
    weights = []
    for class_id in [0, 1]:  # Normal, Pathology
        weight = total_samples / (len(class_counts) * class_counts[class_id])
        weights.append(weight)
    
    return torch.tensor(weights, dtype=torch.float32)



class EarlyStopping:
    """–†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = None
        self.counter = 0
        self.best_weights = None

    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(model)
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            self.save_checkpoint(model)
        else:
            self.counter += 1

        if self.counter >= self.patience:
            if self.restore_best_weights:
                model.load_state_dict(self.best_weights)
            return True
        return False

    def save_checkpoint(self, model):
        self.best_weights = model.state_dict().copy()



def split_data(train_df, test_size=0.2, val_size=0.2, random_state=42):
    """–†–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ train/val/test —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤"""
    print("–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train/val/test...")
    
    # –°–Ω–∞—á–∞–ª–∞ –æ—Ç–¥–µ–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä
    train_val_df, test_df = train_test_split(
        train_df, 
        test_size=test_size, 
        random_state=random_state,
        stratify=train_df['Pathology']
    )
    
    # –ó–∞—Ç–µ–º —Ä–∞–∑–¥–µ–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–∞–Ω–Ω—ã–µ –Ω–∞ train/val
    val_size_adjusted = val_size / (1 - test_size)
    train_df, val_df = train_test_split(
        train_val_df, 
        test_size=val_size_adjusted, 
        random_state=random_state,
        stratify=train_val_df['Pathology']
    )
    
    print(f"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:")
    print(f"‚Ä¢ –û–±—É—á–∞—é—â–∏–π –Ω–∞–±–æ—Ä: {len(train_df)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"‚Ä¢ –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä: {len(val_df)} –æ–±—Ä–∞–∑—Ü–æ–≤") 
    print(f"‚Ä¢ –¢–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä: {len(test_df)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    for name, df in [("Train", train_df), ("Val", val_df), ("Test", test_df)]:
        normal_count = (df['Pathology'] == 0).sum()
        pathology_count = (df['Pathology'] == 1).sum()
        print(f"‚Ä¢ {name} - Normal: {normal_count}, Pathology: {pathology_count}")
    
    return train_df, val_df, test_df


def calculate_auc_during_training(model, dataloader, device):
    """–†–∞—Å—á–µ—Ç AUC –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è"""
    model.eval()
    all_y_true = []
    all_y_pred = []
    
    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ Normal –∫–ª–∞—Å—Å (–∏–Ω–¥–µ–∫—Å 0)
            all_y_true.extend(labels[:, 0].cpu().numpy())
            all_y_pred.extend(outputs[:, 0].cpu().numpy())
    
    if len(np.unique(all_y_true)) > 1:
        return roc_auc_score(all_y_true, all_y_pred)
    else:
        return 0.5



def train_model(config_path: str = "configs/config.yaml"):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ YAML —Ñ–∞–π–ª–∞
    try:
        import yaml
        with open(config_path, 'r', encoding='utf-8') as f:
            yaml_config = yaml.safe_load(f)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ CONFIG –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –∏–∑ YAML
        if 'training' in yaml_config:
            training_config = yaml_config['training']
            
            # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–∏–ø–æ–≤
            def safe_convert(value, default, convert_func):
                if value is None:
                    return default
                try:
                    return convert_func(value)
                except (ValueError, TypeError):
                    return default
            
            CONFIG.update({
                'batch_size': safe_convert(training_config.get('batch_size'), CONFIG['batch_size'], int),
                'epochs': safe_convert(training_config.get('epochs'), CONFIG['epochs'], int),
                'learning_rate': safe_convert(training_config.get('lr'), CONFIG['learning_rate'], float),
                'weight_decay': safe_convert(training_config.get('weight_decay'), CONFIG['weight_decay'], float),
                'early_stopping_patience': safe_convert(training_config.get('early_stopping_patience'), CONFIG['early_stopping_patience'], int),
                'dropout_rate': safe_convert(training_config.get('dropout_rate'), CONFIG['dropout_rate'], float),
                'test_size': safe_convert(training_config.get('test_size'), CONFIG['test_size'], float),
                'val_size': safe_convert(training_config.get('val_size'), CONFIG['val_size'], float),
                'random_state': safe_convert(training_config.get('random_state'), CONFIG['random_state'], int),
                'n_bootstrap': safe_convert(training_config.get('n_bootstrap'), CONFIG['n_bootstrap'], int),
                'confidence_level': safe_convert(training_config.get('confidence_level'), CONFIG['confidence_level'], float),
                'focal_loss_alpha': safe_convert(training_config.get('focal_loss_alpha'), CONFIG['focal_loss_alpha'], float),
                'focal_loss_gamma': safe_convert(training_config.get('focal_loss_gamma'), CONFIG['focal_loss_gamma'], float),
                'class_weights': training_config.get('class_weights', CONFIG['class_weights']),
                'use_weighted_sampler': training_config.get('use_weighted_sampler', CONFIG['use_weighted_sampler']),
                'optimize_threshold': training_config.get('optimize_threshold', CONFIG['optimize_threshold']),
                'series_dir': training_config.get('series_dir', CONFIG['series_dir']),
                'data_file': training_config.get('data_file', CONFIG['data_file'])
            })
        
        print(f"üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑: {config_path}")
        print(f"‚Ä¢ –≠–ø–æ—Ö–∏: {CONFIG['epochs']}")
        print(f"‚Ä¢ Batch size: {CONFIG['batch_size']}")
        print(f"‚Ä¢ Learning rate: {CONFIG['learning_rate']}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ {config_path}: {e}")
        print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–∞–ø–æ–∫
    os.makedirs('plots', exist_ok=True)
    os.makedirs('weights', exist_ok=True)
    os.makedirs('logs', exist_ok=True)
    
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é")
    print("="*80)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    train_df = pd.read_csv("data/training_data.csv")
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(train_df)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω–æ–π –º–µ—Ç–∫–∏ Pathology
    print("üîß –°–æ–∑–¥–∞–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω–æ–π –º–µ—Ç–∫–∏ Pathology...")
    train_df["Pathology"] = 0
    train_df.loc[train_df["Normal"] == 1, "Pathology"] = 0
    train_df.loc[(train_df["COVID"] == 1) | (train_df["Cancer"] == 1), "Pathology"] = 1
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –±–∏–Ω–∞—Ä–Ω—ã–º –∫–ª–∞—Å—Å–∞–º
    normal_count = (train_df['Pathology'] == 0).sum()
    pathology_count = (train_df['Pathology'] == 1).sum()
    print(f"–ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:")
    print(f"  Normal: {normal_count} –∑–∞–ø–∏—Å–µ–π")
    print(f"  Pathology: {pathology_count} –∑–∞–ø–∏—Å–µ–π")
    print(f"  –î–∏—Å–±–∞–ª–∞–Ω—Å: {pathology_count/normal_count:.2f}:1")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    train_df, val_df, test_df = split_data(
        train_df, 
        test_size=CONFIG['test_size'], 
        val_size=CONFIG['val_size'], 
        random_state=CONFIG['random_state']
    )
    
    # –†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤
    class_weights = calculate_class_weights(train_df)
    print(f"–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: Normal={class_weights[0]:.3f}, Pathology={class_weights[1]:.3f}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    print("üèóÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏...")
    processor = DICOMProcessor(TARGET_SIZE)
    model = CNN3D(num_classes=len(LABEL_COLS), dropout_rate=CONFIG['dropout_rate'])
    model.to(CONFIG['device'])
    print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å CNN3D")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = CTDataset(train_df, "data/training_data", processor)
    val_dataset = CTDataset(val_df, "data/training_data", processor)
    test_dataset = CTDataset(test_df, "data/training_data", processor)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    if CONFIG.get('use_weighted_sampler', True):
        weighted_sampler = create_weighted_sampler(train_df)
        train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], sampler=weighted_sampler, num_workers=4)
        print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤")
    else:
        train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=4)
    
    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4)
    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4)
    
    # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
    if CONFIG.get('class_weights', True):
        criterion = nn.BCELoss(weight=class_weights.to(CONFIG['device']))
        print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è BCE Loss —Å –≤–µ—Å–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤")
    else:
        criterion = FocalLoss(
            alpha=CONFIG.get('focal_loss_alpha', 0.75),
            gamma=CONFIG.get('focal_loss_gamma', 2.0)
        )
        print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Focal Loss –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º")
    
    optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)
    early_stopping = EarlyStopping(patience=CONFIG['early_stopping_patience'])
    
    # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
    history = {
        'train_loss': [], 'val_loss': [],
        'train_auc': [], 'val_auc': []
    }
    
    best_val_loss = float('inf')
    best_model_path = 'weights/best_model.pth'
    
    print(f"\nüéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ:")
    print(f"‚Ä¢ –≠–ø–æ—Ö–∏: {CONFIG['epochs']}")
    print(f"‚Ä¢ Batch size: {CONFIG['batch_size']}")
    print(f"‚Ä¢ Learning rate: {CONFIG['learning_rate']}")
    print(f"‚Ä¢ Early stopping: {CONFIG['early_stopping_patience']}")
    
    # –û–±—É—á–µ–Ω–∏–µ
    for epoch in range(CONFIG['epochs']):
        print(f"\nEpoch {epoch + 1}/{CONFIG['epochs']}")
        print("-" * 40)
        
        # –û–±—É—á–µ–Ω–∏–µ
        model.train()
        train_loss = 0.0
        train_samples = 0
        
        train_loop = tqdm(train_loader, desc="–û–±—É—á–µ–Ω–∏–µ", leave=False)
        for images, labels in train_loop:
            images = images.to(CONFIG['device'])
            labels = labels.to(CONFIG['device'])
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item() * images.size(0)
            train_samples += images.size(0)
            
            train_loop.set_postfix({'loss': f"{loss.item():.4f}"})
        
        avg_train_loss = train_loss / train_samples
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        val_loss = 0.0
        val_samples = 0
        
        with torch.no_grad():
            val_loop = tqdm(val_loader, desc="–í–∞–ª–∏–¥–∞—Ü–∏—è", leave=False)
            for images, labels in val_loop:
                images = images.to(CONFIG['device'])
                labels = labels.to(CONFIG['device'])
                
                outputs = model(images)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item() * images.size(0)
                val_samples += images.size(0)
                
                val_loop.set_postfix({'loss': f"{loss.item():.4f}"})
        
        avg_val_loss = val_loss / val_samples
        
        # –†–∞—Å—á–µ—Ç AUC
        train_auc = calculate_auc_during_training(model, train_loader, CONFIG['device'])
        val_auc = calculate_auc_during_training(model, val_loader, CONFIG['device'])
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
        history['train_loss'].append(avg_train_loss)
        history['val_loss'].append(avg_val_loss)
        history['train_auc'].append(train_auc)
        history['val_auc'].append(val_auc)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ scheduler
        scheduler.step(avg_val_loss)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), best_model_path)
            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (val_loss: {best_val_loss:.4f})")
        
        # Early stopping
        if early_stopping(avg_val_loss, model):
            print(f"üõë Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch + 1}")
            break
        
        print(f"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
        print(f"Train AUC: {train_auc:.4f}, Val AUC: {val_auc:.4f}")
    
    print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"‚Ä¢ –õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–æ—Ç–µ—Ä—è: {best_val_loss:.4f}")
    print(f"‚Ä¢ –û–±—É—á–µ–Ω–æ —ç–ø–æ—Ö: {len(history['train_loss'])}")
    

    
    print(f"\nüìä –û—Ü–µ–Ω–∫–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–æ—Ä–æ–≥–∞...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    model.load_state_dict(torch.load(best_model_path))
    
    # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –Ω–∞–±–æ—Ä–æ–≤
    datasets = {
        'train': (train_loader, "–û–±—É—á–∞—é—â–∏–π"),
        'val': (val_loader, "–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π"), 
        'test': (test_loader, "–¢–µ—Å—Ç–æ–≤—ã–π")
    }
    
    all_predictions = {}
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö –Ω–∞–±–æ—Ä–æ–≤
    for dataset_name, (loader, display_name) in datasets.items():
        print(f"\nüîç –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è {display_name.lower()} –Ω–∞–±–æ—Ä–∞:")
        
        model.eval()
        all_y_true = []
        all_y_pred = []
        
        with torch.no_grad():
            for images, labels in tqdm(loader, desc=f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è {display_name.lower()}"):
                images = images.to(CONFIG['device'])
                labels = labels.to(CONFIG['device'])
                
                outputs = model(images)
                all_y_true.extend(labels.cpu().numpy())
                all_y_pred.extend(outputs.cpu().numpy())
        
        all_y_true = np.array(all_y_true)
        all_y_pred = np.array(all_y_pred)
        
        # –î–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ Normal –∫–ª–∞—Å—Å (–∏–Ω–¥–µ–∫—Å 0)
        y_true_binary = all_y_true[:, 0]  # Normal labels
        y_pred_binary = all_y_pred[:, 0]  # Normal predictions
        
        all_predictions[dataset_name] = {
            'y_true': y_true_binary,
            'y_pred': y_pred_binary,
            'display_name': display_name
        }
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –¢–û–õ–¨–ö–û –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
    if CONFIG.get('optimize_threshold', True):
        val_data = all_predictions['val']
        optimal_threshold, optimal_f1 = find_optimal_threshold(val_data['y_true'], val_data['y_pred'])
        print(f"\nüéØ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ (–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏): {optimal_threshold:.4f} (F1: {optimal_f1:.4f})")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        import yaml
        config_path = "configs/config.yaml"
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            config['inference']['threshold'] = float(optimal_threshold)
            
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True, indent=2)
            
            print(f"üíæ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é: {optimal_threshold:.4f}")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Ä–æ–≥ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é: {e}")
    else:
        optimal_threshold = 0.5
        print(f"\nüéØ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ—Ä–æ–≥: {optimal_threshold:.4f}")
    
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ –∫–æ –≤—Å–µ–º –Ω–∞–±–æ—Ä–∞–º
    all_metrics = {}
    
    for dataset_name, data in all_predictions.items():
        y_true_binary = data['y_true']
        y_pred_binary = data['y_pred']
        display_name = data['display_name']
        
        print(f"\nüîç –û—Ü–µ–Ω–∫–∞ –Ω–∞ {display_name.lower()} –Ω–∞–±–æ—Ä–µ:")
        
        # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
        metrics = calculate_metrics_with_threshold(y_true_binary, y_pred_binary, optimal_threshold)
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"\n{'='*60}")
        print(f"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê {display_name.upper()} –ù–ê–ë–û–†–ï")
        print(f"{'='*60}")
        print(f"‚Ä¢ –ü–æ—Ä–æ–≥: {optimal_threshold:.4f}")
        print(f"‚Ä¢ –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {metrics['sensitivity']:.3f}")
        print(f"‚Ä¢ –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å: {metrics['specificity']:.3f}")
        print(f"‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å: {metrics['precision']:.3f}")
        print(f"‚Ä¢ F1-–º–µ—Ä–∞: {metrics['f1_score']:.3f}")
        print(f"‚Ä¢ True Positives: {metrics['tp']}")
        print(f"‚Ä¢ True Negatives: {metrics['tn']}")
        print(f"‚Ä¢ False Positives: {metrics['fp']}")
        print(f"‚Ä¢ False Negatives: {metrics['fn']}")
        print(f"{'='*60}")
        
        # ROC –∫—Ä–∏–≤–∞—è
        fpr, tpr, _ = roc_curve(y_true_binary, y_pred_binary)
        roc_auc = auc(fpr, tpr)
        
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')
        plt.axvline(x=1-optimal_threshold, color='red', linestyle='--', alpha=0.7, label=f'Optimal threshold = {optimal_threshold:.4f}')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate (1 - Specificity)')
        plt.ylabel('True Positive Rate (Sensitivity)')
        plt.title(f'ROC Curve - {display_name} Set')
        plt.legend(loc="lower right")
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig(f'plots/roc_curve_{dataset_name}.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        all_metrics[dataset_name] = {
            'metrics': metrics,
            'y_true': y_true_binary,
            'y_pred': y_pred_binary,
            'optimal_threshold': optimal_threshold,
            'roc_auc': roc_auc
        }
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–µ –≤—ã–≤–æ–¥—ã
    test_metrics = all_metrics['test']['metrics']
    print(f"\n{'='*80}")
    print(f"–§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê –¢–ï–°–¢–û–í–û–ú –ù–ê–ë–û–†–ï")
    print(f"{'='*80}")
    print(f"‚Ä¢ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {all_metrics['test']['optimal_threshold']:.4f}")
    print(f"‚Ä¢ AUC-ROC: {all_metrics['test']['roc_auc']:.3f}")
    print(f"‚Ä¢ –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {test_metrics['sensitivity']:.3f}")
    print(f"‚Ä¢ –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å: {test_metrics['specificity']:.3f}")
    print(f"‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å: {test_metrics['precision']:.3f}")
    print(f"‚Ä¢ F1-–º–µ—Ä–∞: {test_metrics['f1_score']:.3f}")
    print(f"{'='*80}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
    save_training_results(all_metrics, optimal_threshold)
    
    return model, all_metrics



if __name__ == "__main__":
    print("üß† Service4CT - –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø–∞—Ç–æ–ª–æ–≥–∏–π –ö–¢ –æ—Ä–≥–∞–Ω–æ–≤ –≥—Ä—É–¥–Ω–æ–π –∫–ª–µ—Ç–∫–∏")
    print("üìä –° —É–ª—É—á—à–µ–Ω–Ω–æ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–æ—Ä–æ–≥–∞")
    print("="*80)
    
    try:
        model, metrics = train_model()
        print("\nüéâ –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()